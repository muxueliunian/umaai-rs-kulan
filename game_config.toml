# ---- 以下参数对模拟和AI模式均生效，请根据实际情况调整 ----

# 种马额外属性 [速度, 耐力, 力量, 根性, 智力, 技能点]
extra_count = [10, 0, 0, 40, 40, 50]

# 温泉选择顺序 
# 疾驰之泉(1) - 速度/力量友情
# 坚忍之泉(2) - 耐力/根性友情
# 明晰之泉(3) - 比赛+30%
# 骏闪古泉(4) - Hint+100%
# 刚足古泉(5) - 力量/根性友情\
# 健壮古泉(6) - 耐力/体力消耗
# 天翔古泉(7) - 比赛+60%
# 秘汤汤驹(8) - 分身效果
# 传说秘泉(9) - 比赛+80%（⭐特殊：第三年9月强制选择）
onsen_order = [
    1, 3, 2,    # 第一年
    7, 6, 2,   # 第二年
    8, 9,       # 第三年
    4, 5     # 补漏用
]

# 蒙特卡洛配置（仅当 trainer = "mcts" 时生效）
#   search_n              - 搜索次数，每个动作模拟次数（默认10240，太慢的话可选2048,4096）
#   radical_factor_max    - 激进度上限，越高越倾向可能有隐藏高回报的“表面昏招”（默认2）
#   max_depth             - 最大搜索深度，0=搜到游戏结束（默认0）
#   policy_delta          - 策略温度（训练数据生成时使用，默认100）
#
# UCB 搜索分配参数 
#   use_ucb               - 是否启用 UCB 分配（默认 true）
#   search_group_size     - UCB 每次给动作增加的搜索次数（默认256，C++版是1024）
#   search_cpuct          - UCB 探索常数，越大越倾向探索搜索次数少的动作（默认1.0）
#   expected_search_stdev - 预期搜索标准差（默认2200，C++版的常量）
#
#
#   ⬇️🐧⭐ toml 的内联表{ ..... }不支持换行 ，换行会报错。
mcts = { search_n = 10240, radical_factor_max = 2.0, max_depth = 0, policy_delta = 100.0, use_ucb = true, search_group_size = 512 }

# 日志级别: "debug" (完整显示) | "off" (全部关闭) | "info" (简要显示) | "trace" (详细显示) 
log_level = "info"

# ---- 以下参数仅模拟中生效 ----
# 模拟次数（默认1次，大于1时显示最高分/最低分面板和平均分）

simulation_count = 1

# 马娘ID
uma = 106301

# 卡组(ID, 突破等级)
# 温泉剧本需要包含友人卡 (chara_id = 9050)
# 21110
#cards = [302754, 302654, 302744, 302644, 302774, 302764]
# 20111
#cards = [302754, 302654, 302484, 302644, 302774, 302764]
# 32000
cards = [302754, 302654, 302814, 302794, 302424, 302764]

# 种马蓝因子个数 [速度, 耐力, 力量, 根性, 智力]
blue_count = [15, 0, 3, 0, 0]

# ---- 以下参数勿动 ----
scenario = "onsen"

# 训练员类型:
#   "manual"      - 手动选择（交互式，不支持多次模拟）
#   "random"      - 猴子训练员（随机选择，用于基准测试）
#   "handwritten" - 手写策略（快速，使用启发式规则）
#   "collector"   - 样本收集器（用于生成神经网络训练数据）
#   "neuralnet"   - 神经网络训练员（使用 ONNX 模型进行决策）
#   "mcts"        - 蒙特卡洛训练员（使用手写逻辑+mcts进行决策）
trainer = "mcts"








